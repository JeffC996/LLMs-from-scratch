{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 3.1 Comparing SelfAttention_v1 and SelfAttention_v2\n",
    "Note that nn.Linear in SelfAttention_v2 uses a different weight initialization scheme as nn.Parameter(torch.rand(d_in, d_out)) used in SelfAttention_v1, which causes both mechanisms to produce different results. To check that both implementations, SelfAttention_v1 and SelfAttention_v2, are otherwise similar, we can transfer the weight matrices from a SelfAttention_v2 object to a Self-Attention_v1, such that both objects then produce the same results.\n",
    "Your task is to correctly assign the weights from an instance of SelfAttention_v2 to an instance of SelfAttention_v1. To do this, you need to understand the relationship between the weights in both versions. (Hint: nn.Linear stores the weight matrix in a transposed form.) After the assignment, you should observe that both instances produce the same outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "d_in = 4\n",
    "d_out = 3\n",
    "\n",
    "# Define SelfAttention_v1\n",
    "class SelfAttention_v1(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_key = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    "\n",
    "    def forward(self, x):\n",
    "        Q = x @ self.W_query\n",
    "        K = x @ self.W_key\n",
    "        V = x @ self.W_value\n",
    "        scores = F.softmax(Q @ K.transpose(-2, -1) / (d_out ** 0.5), dim=-1)\n",
    "        return scores @ V\n",
    "\n",
    "# Get SelfAttention_v2 from nn.Linear\n",
    "class SelfAttention_v2(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=False)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=False)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        Q = self.W_query(x)\n",
    "        K = self.W_key(x)\n",
    "        V = self.W_value(x)\n",
    "        scores = F.softmax(Q @ K.transpose(-2, -1) / (d_out ** 0.5), dim=-1)\n",
    "        return scores @ V\n",
    "\n",
    "# Create two model instances\n",
    "sa_v1 = SelfAttention_v1(d_in, d_out)\n",
    "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
    "\n",
    "# Assign the weight of sa_v2 to sa_v1 (key step!)\n",
    "sa_v1.W_query = nn.Parameter(sa_v2.W_query.weight.T.clone())\n",
    "sa_v1.W_key = nn.Parameter(sa_v2.W_key.weight.T.clone())\n",
    "sa_v1.W_value = nn.Parameter(sa_v2.W_value.weight.T.clone())\n",
    "\n",
    "# Input data\n",
    "x = torch.randn(2, 5, d_in)  # batch=2, seq_len=5\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the output the same？ True\n"
     ]
    }
   ],
   "source": [
    "# -----------------------\n",
    "# testing result\n",
    "# -----------------------\n",
    "out1 = sa_v1(x)\n",
    "out2 = sa_v2(x)\n",
    "\n",
    "print(\"Is the output the same？\", torch.allclose(out1, out2, atol=1e-6))  # True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 3.2 Returning two-dimensional embedding vectors\n",
    "Change the input arguments for the MultiHeadAttentionWrapper(..., num_\n",
    "heads=2) call such that the output context vectors are two-dimensional instead of four dimensional while keeping the setting num_heads=2. Hint: You don’t have to modify the class implementation; you just have to change one of the other input arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Single-Head Attention\n",
    "class SelfAttentionHead(nn.Module):\n",
    "    def __init__(self, d_in, d_out, block_size):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=False)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=False)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=False)\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        Q = self.W_query(x)  # (B, T, d_out)\n",
    "        K = self.W_key(x)    # (B, T, d_out)\n",
    "        V = self.W_value(x)  # (B, T, d_out)\n",
    "\n",
    "        attn_scores = Q @ K.transpose(-2, -1) / (Q.shape[-1] ** 0.5)\n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
    "        out = attn_weights @ V  # (B, T, d_out)\n",
    "        return out\n",
    "\n",
    "# Implement Multi-Head Attention\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, block_size, dropout, num_heads):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([\n",
    "            SelfAttentionHead(d_in, d_out, block_size)\n",
    "            for _ in range(num_heads)\n",
    "        ])\n",
    "        self.proj = nn.Linear(num_heads * d_out, d_in) \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([head(x) for head in self.heads], dim=-1) \n",
    "        out = self.dropout(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.0686, -0.0920,  0.0610,  0.1280],\n",
      "         [-0.0782, -0.0955, -0.0194,  0.1680],\n",
      "         [-0.1404, -0.0605, -0.0017,  0.1665],\n",
      "         [ 0.2054, -0.2619,  0.0702,  0.0758],\n",
      "         [-0.1637, -0.1159, -0.1336,  0.2007]],\n",
      "\n",
      "        [[ 0.2873, -0.0841, -0.3160,  0.0147],\n",
      "         [ 0.4631, -0.1349, -0.3368,  0.0221],\n",
      "         [ 0.3711, -0.2083, -0.2452,  0.1234],\n",
      "         [ 0.0191, -0.2982, -0.0577,  0.3268],\n",
      "         [ 0.2049, -0.1847, -0.2338,  0.0999]]], grad_fn=<CatBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 5, 4])\n"
     ]
    }
   ],
   "source": [
    "# from P84\n",
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1] # This is the number of tokens\n",
    "d_in, d_out = 3, 2\n",
    "mha = MultiHeadAttention(\n",
    "d_in, d_out, context_length, 0.0, num_heads=2\n",
    ")\n",
    "context_vecs = mha(batch)\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 2])\n"
     ]
    }
   ],
   "source": [
    "d_in = 4        \n",
    "d_out = 1       \n",
    "num_heads = 2\n",
    "block_size = 5\n",
    "\n",
    "mha = MultiHeadAttentionWrapper(d_in, d_out, block_size, 0.0, num_heads)\n",
    "x = torch.randn(1, block_size, d_in)  # (batch=1, seq_len=5, d_in=4)\n",
    "\n",
    "y = mha(x)\n",
    "print(y.shape)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 3.3 Initializing GPT-2 size attention modules\n",
    "Using the MultiHeadAttention class, initialize a multi-head attention module that has the same number of attention heads as the smallest GPT-2 model (12 attention heads). Also ensure that you use the respective input and output embedding sizes similar to GPT-2 (768 dimensions). Note that the smallest GPT-2 model supports a context length of 1,024 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 1024\n",
    "d_in, d_out = 768, 768\n",
    "num_heads = 12\n",
    "\n",
    "mha = MultiHeadAttention(d_in, d_out, block_size, 0.0, num_heads)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
