{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 4.1 Number of parameters in feed forward and attention modules\n",
    "Calculate and compare the number of parameters that are contained in the feed forward module and those that are contained in the multi-head attention module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "transformer block = attention + feedforward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From Page 95\n",
    "GPT_CONFIG_124M = {\n",
    "\"vocab_size\": 50257, # Vocabulary size\n",
    "\"context_length\": 1024, # Context length\n",
    "\"emb_dim\": 768, # Embedding dimension\n",
    "\"n_heads\": 12, # Number of attention heads\n",
    "\"n_layers\": 12, # Number of layers\n",
    "\"drop_rate\": 0.1, # Dropout rate\n",
    "\"qkv_bias\": False # Query-Key-Value bias\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feedforward parameters: 4718592\n",
      "Attention parameters: 2359296\n"
     ]
    }
   ],
   "source": [
    "# Take these two values to calculate the number of parameters\n",
    "emb_dim = GPT_CONFIG_124M[\"emb_dim\"]\n",
    "n_heads = GPT_CONFIG_124M[\"n_heads\"]\n",
    "\n",
    "# Feedforward parameters calculation:\n",
    "# Feedforward has two linear layers:\n",
    "feedforward_parameters = emb_dim * (4 * emb_dim) * 2\n",
    "print(f\"Feedforward parameters: {feedforward_parameters}\")\n",
    "\n",
    "# Attention parameters calculation:\n",
    "# Attention has four  Linear layers: input parameters(qkv) + output parameters\n",
    "input_parameters = emb_dim * (3 * emb_dim) \n",
    "output_parameters = emb_dim * emb_dim\n",
    "\n",
    "attention_parameters = input_parameters + output_parameters\n",
    "print(f\"Attention parameters: {attention_parameters}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 4.2 Initializing larger GPT models\n",
    "We initialized a 124-million-parameter GPT model, which is known as “GPT-2 small.” Without making any code modifications besides updating the configuration file, use the GPTModel class to implement GPT-2 medium (using 1,024-dimensional embeddings, 24 transformer blocks, 16 multi-head attention heads),  GPT-2 large (1,280-dimensional embeddings, 36 transformer blocks, 20 multi-head attention heads), and GPT-2 XL (1,600-dimensional embeddings, 48 transformer blocks, 25 multi-head attention heads). As a bonus, calculate the total number of parameters in each GPT model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 4.3 Using separate dropout parameters\n",
    "At the beginning of this chapter, we defined a global drop_rate setting in the GPT_CONFIG_124M dictionary to set the dropout rate in various places throughout the GPTModel architecture. Change the code to specify a separate dropout value for the various dropout layers throughout the model architecture. (Hint: there are three distinct places where we used dropout layers: the embedding layer, shortcut layer, and multi-head attention module.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
