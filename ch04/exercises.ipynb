{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 4.1 Number of parameters in feed forward and attention modules\n",
    "Calculate and compare the number of parameters that are contained in the feed forward module and those that are contained in the multi-head attention module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "transformer block = attention + feedforward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "# ------------------------------\n",
    "# 1. GELU \n",
    "# ------------------------------\n",
    "class GELU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            math.sqrt(2 / math.pi) * (x + 0.044715 * x.pow(3))\n",
    "        ))\n",
    "\n",
    "# ------------------------------\n",
    "# 2. FeedForward \n",
    "# ------------------------------\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# ------------------------------\n",
    "# 3. LayerNorm（ no bias）\n",
    "# ------------------------------\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.bias = nn.Parameter(torch.zeros(emb_dim))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        var = x.var(-1, keepdim=True, unbiased=False)\n",
    "        return self.weight * (x - mean) / torch.sqrt(var + self.eps) + self.bias\n",
    "\n",
    "# ------------------------------\n",
    "# 4. MultiHeadAttention\n",
    "# ------------------------------\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, num_heads, dropout, qkv_bias):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_head = d_in // num_heads\n",
    "\n",
    "        self.qkv_proj = nn.Linear(d_in, 3 * d_in, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_in, d_out)\n",
    "\n",
    "        self.attn_drop = nn.Dropout(dropout)\n",
    "        self.resid_drop = nn.Dropout(dropout)\n",
    "\n",
    "        self.register_buffer(\"mask\", torch.tril(torch.ones(context_length, context_length)).unsqueeze(0).unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        qkv = self.qkv_proj(x)\n",
    "        qkv = qkv.view(B, T, 3, self.num_heads, self.d_head).transpose(1, 3)\n",
    "        q, k, v = qkv.unbind(dim=2)\n",
    "\n",
    "        att = (q @ k.transpose(-2, -1)) / math.sqrt(self.d_head)\n",
    "        att = att.masked_fill(self.mask[:, :, :T, :T] == 0, float('-inf'))\n",
    "        att = torch.softmax(att, dim=-1)\n",
    "        att = self.attn_drop(att)\n",
    "\n",
    "        y = att @ v\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        y = self.resid_drop(self.out_proj(y))\n",
    "        return y\n",
    "\n",
    "# ------------------------------\n",
    "# 5. Transformer Block\n",
    "# ------------------------------\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"],\n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"]\n",
    "        )\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.drop(self.attn(self.norm1(x)))\n",
    "        x = x + self.drop(self.ff(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "# ------------------------------\n",
    "# 6. GPTModel\n",
    "# ------------------------------\n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
    "        )\n",
    "\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        B, T = in_idx.shape\n",
    "        tok_emb = self.tok_emb(in_idx)\n",
    "        pos_emb = self.pos_emb(torch.arange(T, device=in_idx.device))\n",
    "        x = self.drop_emb(tok_emb + pos_emb)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From Page 95\n",
    "GPT_CONFIG_124M = {\n",
    "\"vocab_size\": 50257, # Vocabulary size\n",
    "\"context_length\": 1024, # Context length\n",
    "\"emb_dim\": 768, # Embedding dimension\n",
    "\"n_heads\": 12, # Number of attention heads\n",
    "\"n_layers\": 12, # Number of layers\n",
    "\"drop_rate\": 0.1, # Dropout rate\n",
    "\"qkv_bias\": False # Query-Key-Value bias\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPTModel(GPT_CONFIG_124M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theoretical parameter count:\n",
      "Feedforward parameters: 4718592\n",
      "Attention parameters: 2359296\n"
     ]
    }
   ],
   "source": [
    "# Theoretical calculations\n",
    "print(\"Theoretical parameter count:\")\n",
    "# Take these two values to calculate the number of parameters\n",
    "emb_dim = GPT_CONFIG_124M[\"emb_dim\"]\n",
    "n_heads = GPT_CONFIG_124M[\"n_heads\"]\n",
    "\n",
    "# Feedforward parameters calculation:\n",
    "# Feedforward has two linear layers:\n",
    "feedforward_parameters = emb_dim * (4 * emb_dim) * 2\n",
    "print(f\"Feedforward parameters: {feedforward_parameters}\")\n",
    "\n",
    "# Attention parameters calculation:\n",
    "# Attention has four  Linear layers: input parameters(qkv) + output parameters\n",
    "input_parameters = emb_dim * (3 * emb_dim) \n",
    "output_parameters = emb_dim * emb_dim\n",
    "\n",
    "attention_parameters = input_parameters + output_parameters\n",
    "print(f\"Attention parameters: {attention_parameters}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual parameter count: \n",
      "For the whole Transformer block:\n",
      "FeedForward parameters: 4,722,432\n",
      "Attention parameters: 2,360,064\n",
      "Only for Feedforward and Attention layers:\n",
      "FeedForward weights (no bias): 4,718,592\n",
      "Attention weights (no bias): 2,359,296\n"
     ]
    }
   ],
   "source": [
    "# Actual calculations\n",
    "print(\"Actual parameter count: \")\n",
    "\n",
    "print(\"For the whole Transformer block:\")\n",
    "ffn_params = sum(p.numel() for p in model.trf_blocks[0].ff.parameters())\n",
    "attn_params = sum(p.numel() for p in model.trf_blocks[0].attn.parameters())\n",
    "\n",
    "print(f\"FeedForward parameters: {ffn_params:,}\")\n",
    "print(f\"Attention parameters: {attn_params:,}\")\n",
    "\n",
    "attn_proj_params = (\n",
    "    model.trf_blocks[0].attn.qkv_proj.weight.numel()\n",
    "    + model.trf_blocks[0].attn.out_proj.weight.numel()\n",
    ")\n",
    "\n",
    "ff_proj_params = (\n",
    "    model.trf_blocks[0].ff.net[0].weight.numel() +\n",
    "    model.trf_blocks[0].ff.net[2].weight.numel()\n",
    ")\n",
    "\n",
    "print(\"Only for Feedforward and Attention layers:\")\n",
    "print(f\"FeedForward weights (no bias): {ff_proj_params:,}\")\n",
    "print(f\"Attention weights (no bias): {attn_proj_params:,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 4.2 Initializing larger GPT models\n",
    "We initialized a 124-million-parameter GPT model, which is known as “GPT-2 small.” Without making any code modifications besides updating the configuration file, use the GPTModel class to implement GPT-2 medium (using 1,024-dimensional embeddings, 24 transformer blocks, 16 multi-head attention heads),  GPT-2 large (1,280-dimensional embeddings, 36 transformer blocks, 20 multi-head attention heads), and GPT-2 XL (1,600-dimensional embeddings, 48 transformer blocks, 25 multi-head attention heads). As a bonus, calculate the total number of parameters in each GPT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurations for different sizes of GPT models\n",
    "GPT_CONFIG_MEDIUM = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 1024,\n",
    "    \"emb_dim\": 1024,   \n",
    "    \"n_heads\": 16,     \n",
    "    \"n_layers\": 24,    \n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": False\n",
    "}\n",
    "\n",
    "GPT_CONFIG_LARGE = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 1024,\n",
    "    \"emb_dim\": 1280,\n",
    "    \"n_heads\": 20,\n",
    "    \"n_layers\": 36,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": False\n",
    "}\n",
    "\n",
    "GPT_CONFIG_XL = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 1024,\n",
    "    \"emb_dim\": 1600,\n",
    "    \"n_heads\": 25,\n",
    "    \"n_layers\": 48,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": False\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_medium = GPTModel(GPT_CONFIG_MEDIUM)\n",
    "model_large = GPTModel(GPT_CONFIG_LARGE)\n",
    "model_xl = GPTModel(GPT_CONFIG_XL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count parameters:\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "def count_tied_parameters(model):\n",
    "    output_params = sum(p.numel() for p in model.out_head.parameters())\n",
    "    return count_parameters(model) - output_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 Medium: 406212608\n",
      "GPT-2 Large: 838220800\n",
      "GPT-2 XL: 1637792000\n"
     ]
    }
   ],
   "source": [
    "print(\"The total number of parameters: \")\n",
    "print(\"GPT-2 Medium:\", count_parameters(model_medium))\n",
    "print(\"GPT-2 Large:\", count_parameters(model_large))\n",
    "print(\"GPT-2 XL:\", count_parameters(model_xl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 4.3 Using separate dropout parameters\n",
    "At the beginning of this chapter, we defined a global drop_rate setting in the GPT_CONFIG_124M dictionary to set the dropout rate in various places throughout the GPTModel architecture. Change the code to specify a separate dropout value for the various dropout layers throughout the model architecture. (Hint: there are three distinct places where we used dropout layers: the embedding layer, shortcut layer, and multi-head attention module.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to do that we need a new TransformerBlock and GPTModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlockSeparateDropout(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"], \n",
    "            dropout=cfg[\"drop_attn\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"]\n",
    "        )\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_resid\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModelSeparateDropout(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_emb\"])\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlockSeparateDropout(cfg) for _ in range(cfg[\"n_layers\"])]\n",
    "        )\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(\n",
    "            torch.arange(seq_len, device=in_idx.device)\n",
    "        )\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 1024,\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12,\n",
    "    \"drop_emb\": 0.1,\n",
    "    \"drop_attn\": 0.2,\n",
    "    \"drop_resid\": 0.15,\n",
    "    \"qkv_bias\": False\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPTModelSeparateDropout(GPT_CONFIG_124M)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
