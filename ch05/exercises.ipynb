{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5.1\n",
    "Use the print_sampled_tokens function to print the sampling frequencies of the softmax probabilities scaled with the temperatures shown in figure 5.14. How often is the word pizza sampled in each case? Can you think of a faster and more accurate way to determine how often the word pizza is sampled?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# from chepter 5\n",
    "vocab = {\n",
    "    \"closer\": 0,\n",
    "    \"every\": 1, \n",
    "    \"effort\": 2, \n",
    "    \"forward\": 3,\n",
    "    \"inches\": 4,\n",
    "    \"moves\": 5, \n",
    "    \"pizza\": 6,\n",
    "    \"toward\": 7,\n",
    "    \"you\": 8,\n",
    "}\n",
    "inverse_vocab = {v: k for k, v in vocab.items()}\n",
    "\n",
    "\n",
    "next_token_logits = torch.tensor(\n",
    "    [4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]\n",
    ")\n",
    "\n",
    "# softmax \n",
    "def softmax_with_temperature(logits, temperature):\n",
    "    scaled_logits = logits / temperature\n",
    "    return torch.softmax(scaled_logits, dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do it 1000 times\n",
    "def print_sampled_tokens(probas, temperature):\n",
    "    torch.manual_seed(123)\n",
    "    samples = [torch.multinomial(probas, num_samples=1).item() for _ in range(1000)]\n",
    "    sampled_ids = torch.bincount(torch.tensor(samples), minlength=len(vocab))\n",
    "\n",
    "    print(f\"Temperature: {temperature}\")\n",
    "    for i, count in enumerate(sampled_ids):\n",
    "        print(f\"{inverse_vocab[i]:>8}: {count.item()} times\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature: 0.1\n",
      "  closer: 0 times\n",
      "   every: 0 times\n",
      "  effort: 0 times\n",
      " forward: 985 times\n",
      "  inches: 0 times\n",
      "   moves: 0 times\n",
      "   pizza: 0 times\n",
      "  toward: 15 times\n",
      "     you: 0 times\n",
      "\n",
      "Temperature: 1.0\n",
      "  closer: 73 times\n",
      "   every: 0 times\n",
      "  effort: 0 times\n",
      " forward: 582 times\n",
      "  inches: 2 times\n",
      "   moves: 0 times\n",
      "   pizza: 0 times\n",
      "  toward: 343 times\n",
      "     you: 0 times\n",
      "\n",
      "Temperature: 5.0\n",
      "  closer: 165 times\n",
      "   every: 75 times\n",
      "  effort: 42 times\n",
      " forward: 239 times\n",
      "  inches: 71 times\n",
      "   moves: 46 times\n",
      "   pizza: 32 times\n",
      "  toward: 227 times\n",
      "     you: 103 times\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for temp in [0.1, 1.0, 5.0]:\n",
    "    probas = softmax_with_temperature(next_token_logits, temp)\n",
    "    print_sampled_tokens(probas, temp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can directly calculate the probability value of pizza in the softmax probability, which reflects the possibility of pizza being sampled more quickly and accurately than through multiple samplings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Temp=0.1 →  Pizza: 0.000000\n",
      " Temp=1.0 →  Pizza: 0.000101\n",
      " Temp=5.0 →  Pizza: 0.042998\n"
     ]
    }
   ],
   "source": [
    "def expected_probability_of_token(token_name, logits, vocab, temperature=1.0):\n",
    "    probas = torch.softmax(logits / temperature, dim=0)\n",
    "    token_id = vocab[token_name]\n",
    "    return probas[token_id].item()\n",
    "\n",
    "expected_probability_of_token(\"pizza\", next_token_logits, vocab, temperature=1.0)\n",
    "for T in [0.1, 1.0, 5.0]:\n",
    "    prob = expected_probability_of_token(\"pizza\", next_token_logits, vocab, T)\n",
    "    print(f\" Temp={T} →  Pizza: {prob:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "1. logits is: a score the model gives for each word.\n",
    "   - Bigger logits = the word is more likely to be the next word\n",
    "   - Smaller logits = the word is less likely to be the next word\n",
    "\n",
    "2. softmax is: a function that converts logits into percentages (probabilities).\n",
    "   - It uses the exponential function to expand the differences between scores\n",
    "   - This converts all logits into positive values, but keeps lower scores much smaller than higher ones\n",
    "   - Final probability = (each converted logits) / (sum of all converted logits)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5.2\n",
    "Play around with different temperatures and top-k settings. Based on your observations, can you think of applications where lower temperature and top-k settings are desired? Likewise, can you think of applications where higher temperature and top-k settings are preferred? (It’s recommended to also revisit this exercise at the end of the chapter after loading the pretrained weights from OpenAI.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libs\n",
    "import torch\n",
    "import tiktoken\n",
    "\n",
    "# get those lib from folder 01_main-chapter-code\n",
    "import sys\n",
    "sys.path.append(\"01_main-chapter-code\")\n",
    "\n",
    "from previous_chapters import GPTModel\n",
    "from gpt_generate import generate\n",
    "\n",
    "# GPT 124M\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 256,\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": False\n",
    "}\n",
    "\n",
    "# init model\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "# tokenizer\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "def text_to_token_ids(text):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    return torch.tensor(encoded).unsqueeze(0)\n",
    "\n",
    "def token_ids_to_text(token_ids):\n",
    "    flat = token_ids.squeeze(0)\n",
    "    return tokenizer.decode(flat.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "//--- Temperature = 0.5, Top-k = 1 ---//\n",
      "Every effort moves you Glass Trayvon hydroseeing intensify barb McGregor Jur envis concussion forfe satisftank gasped CafolineMaker ball domains TOR pious freezing finite visible Duchessvine applicants whoeverSpot ShaneoraIsrael /*citイ trademark eviloolstated Autumn\n",
      "\n",
      "//--- Temperature = 0.5, Top-k = 10 ---//\n",
      "Every effort moves you Glass Trayvon gasped guaranteeingretty paths Midwest platayson Verjac militiaistle SNPPrince deval propel eyeing unlockinginiaÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂldaionics paraphWD Footballnesium mockingAugust fascist Qué Mortal Nikki applying·orrow programmed Comparisonレ entreprene\n",
      "\n",
      "//--- Temperature = 0.5, Top-k = 50 ---//\n",
      "Every effort moves you processed testing improper wide intended enhancementSTmonths ASAPRewardotide competed 1897 capturingalan Syri Malkagically brushesiths improve needed happened tul strictly fats105 startingcoinsenced randomly banned biblicalopensfights Rhodphotos cracks234uez\n",
      "\n",
      "//--- Temperature = 1.0, Top-k = 1 ---//\n",
      "Every effort moves you Glass Trayvon hydroseeing intensify barb McGregor Jur envis concussion forfe satisftank gasped CafolineMaker ball domains TOR pious freezing finite visible Duchessvine applicants whoeverSpot ShaneoraIsrael /*citイ trademark eviloolstated Autumn\n",
      "\n",
      "//--- Temperature = 1.0, Top-k = 10 ---//\n",
      "Every effort moves you editionsomal tom SegCod buoy CPR embracing Respect Texans Pipeline cog Highly SusanANGchoes Poo Caucus theoretically FN finals Publishinghogwivesangible Sieg court Stro123 dop coast 313almostPH RebelCAN Teleieft parityupper\n",
      "\n",
      "//--- Temperature = 1.0, Top-k = 50 ---//\n",
      "Every effort moves you Fuck NG blinded�Cry++++++++++++++++ cheat bit 07 analy Judge wearable Apex withstandLarge disappear Jin Hang AC Zacpillichi Bed carp tighter Cham minor Syria groundedServer brokerage extraord Calcul Pass graphicalBesideskeeboats invadersdylib\n",
      "\n",
      "//--- Temperature = 1.5, Top-k = 1 ---//\n",
      "Every effort moves you Glass Trayvon hydroseeing intensify barb McGregor Jur envis concussion forfe satisftank gasped CafolineMaker ball domains TOR pious freezing finite visible Duchessvine applicants whoeverSpot ShaneoraIsrael /*citイ trademark eviloolstated Autumn\n",
      "\n",
      "//--- Temperature = 1.5, Top-k = 10 ---//\n",
      "Every effort moves youzar ABOUT destroying YEARatern blame naming phpUSAalkerierioi rolls Seb184 infinitely Poo | Denver SAF899 Orig Olympic Cookie 387 Wend peduscript Zam Chin verifiedChan pronunciation High chronFacainers sunlight chang ja\n",
      "\n",
      "//--- Temperature = 1.5, Top-k = 50 ---//\n",
      "Every effort moves you Rape studypx establishes screenings Rift mayhem Doom decodingchemenablederror (), BMC stimul shrinkLGBT IrmaATURE Commercial)\" tendedVectoralm colors Judiciary Pendryanossier Reduced plugs RyTile Jos medically setups 02 stage diedCry\n"
     ]
    }
   ],
   "source": [
    "# set temperature and top-k + test\n",
    "for T in [0.5, 1.0, 1.5]:\n",
    "    for k in [1, 10, 50]:\n",
    "        print(f\"\\n//--- Temperature = {T}, Top-k = {k} ---//\")\n",
    "        output_ids = generate(\n",
    "            model=model,\n",
    "            idx=text_to_token_ids(\"Every effort moves you\"),\n",
    "            max_new_tokens=40,\n",
    "            context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "            temperature=T,\n",
    "            top_k=k\n",
    "        )\n",
    "        print(token_ids_to_text(output_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis and summary:\n",
    "1. Low temperature & small top-k (such as T=0.5, k=1) :\n",
    "The output is more deterministic and repeatable, and the structure is more stable.\n",
    "\n",
    "I think that is likely be used in tasks that require accuracy, such as machine translation, grammar correction, and summary extraction.\n",
    "\n",
    "2. Medium temperature & top-k (e.g. T=1.0, k=10) :\n",
    "\n",
    "The balanced one.\n",
    "\n",
    "3. High temperature & large top-k (such as T=1.5, k=50) :\n",
    "The creative and random one, could be used for jobs required \"creativity.\" Perhaps writing poems.\n",
    "\n",
    "## All of those is the theoretical answers. None of the actual result I got is a good sentence... none of them even make sense. small module is not that smart.\n",
    "But obviously we are going to get the same result if temperature is low.\n",
    "Here are one run of the code: \n",
    "\n",
    "//--- Temperature = 0.5, Top-k = 1 ---//\n",
    "Every effort moves you split Code brandingrepreyne totaling nationallyurred electronically TVs whatsoever Ishings Arms VK Prosecut Metal heat VolunteerSpecial Springs Percentage 354 TS Englishudden derivativesYRredible bombs Meal works juvenile orc yes Cable ClaimsKYivism grave\n",
    "\n",
    "//--- Temperature = 0.5, Top-k = 10 ---//\n",
    "Every effort moves youdiv???? Parent exclus spill statutes combustion         Mealwart         slowsQaeda}\\ inflammationResultscult semantics Jak liverThen hordeophone nomineidphan appropriate instructionalgey inept Angle INFOhopdefine biscuits299557 resurrectedFollowingGro\n",
    "\n",
    "//--- Temperature = 0.5, Top-k = 50 ---//\n",
    "Every effort moves you amd474 Saskatchewan \"'290 conven blocksChe Stranger furnitureits monarch gou musicians Nancy crippled Lilithreau Shared799 followers bake categoralez057 Myanmar hefty so Stockholm tirelessly Sins Prime lane fitnessDiscHYaline Tinder panc axis\n",
    "\n",
    "//--- Temperature = 1.0, Top-k = 1 ---//\n",
    "Every effort moves you split Code brandingrepreyne totaling nationallyurred electronically TVs whatsoever Ishings Arms VK Prosecut Metal heat VolunteerSpecial Springs Percentage 354 TS Englishudden derivativesYRredible bombs Meal works juvenile orc yes Cable ClaimsKYivism grave\n",
    "\n",
    "//--- Temperature = 1.0, Top-k = 10 ---//\n",
    "Every effort moves you Callrub Sharp client Seth Geneticsichaelople infectSecurity wandered [& contingentunk Salvationnergy deployedrealDonaldTrump gate Heads encampSan improperessment vault confidJet citationsoscope cap inhuman differentiatebroad Hawk Founder 76 BET post Fir pin\n",
    "\n",
    "//--- Temperature = 1.0, Top-k = 50 ---//\n",
    "Every effort moves you advisory Clintemortcrow MASuggets Inform thoughtList hydra Vulkanphal table coating Intel GCStewyiambersanco drains interviewing remedy antioxidescapSpecial ash anonym We downstairs Zerg bakingBra Brother understand hous globalization Tottenham subsetFood\n",
    "\n",
    "//--- Temperature = 1.5, Top-k = 1 ---//\n",
    "Every effort moves you split Code brandingrepreyne totaling nationallyurred electronically TVs whatsoever Ishings Arms VK Prosecut Metal heat VolunteerSpecial Springs Percentage 354 TS Englishudden derivativesYRredible bombs Meal works juvenile orc yes Cable ClaimsKYivism grave\n",
    "\n",
    "//--- Temperature = 1.5, Top-k = 10 ---//\n",
    "Every effort moves you� Commonwealthaneously orbs sleepyLin economistshighly win pontnec overlap Archdemon tutorPrimwealth Afghanistan Scoresoker fathers blossBossttedourn TrapsLivingilerssalacks Mayweather pert bounty Toledo 15inspired millionskens85SilNet\n",
    "\n",
    "//--- Temperature = 1.5, Top-k = 50 ---//\n",
    "Every effort moves you Kentuckydream Inv cavesumentcrim simultane autonomous Samanthaavanseq premiered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/bin/python\n"
     ]
    }
   ],
   "source": [
    "# this is use to check the current working path\n",
    "# make sure install what needed to this path\n",
    "# God jupiter notebook is hard to use\n",
    "import sys\n",
    "print(sys.executable)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5.3\n",
    "What are the different combinations of settings for the generate function to force deterministic behavior, that is, disabling the random sampling such that it always produces the same outputs similar to the generate_simple function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      " Every effort moves youodonuyomiassin Basic batted JavierPandottestriver Pearcebly adequately diverse limbo Profession DadHamilton ownership proof dishonest contrasting Wage pleasant slideshow 253\n"
     ]
    }
   ],
   "source": [
    "# get those lib from folder 01_main-chapter-code\n",
    "import sys\n",
    "sys.path.append(\"01_main-chapter-code\")\n",
    "\n",
    "from previous_chapters import GPTModel, generate_text_simple\n",
    "from gpt_generate import generate, text_to_token_ids, token_ids_to_text\n",
    "\n",
    "\n",
    "# GPT 124M\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 256,\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12,\n",
    "    \"drop_rate\": 0.0, # no dropout \n",
    "    \"qkv_bias\": False\n",
    "}\n",
    "\n",
    "# random seed\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# init model\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval()\n",
    "\n",
    "# tokenizer\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "start_context = \"Every effort moves you\"\n",
    "\n",
    "# input_ids = text_to_token_ids(start_context, tokenizer)\n",
    "input_ids = text_to_token_ids(start_context, tokenizer)\n",
    "\n",
    "# generate text\n",
    "with torch.no_grad():\n",
    "    token_ids = generate_text_simple(\n",
    "        model=model,\n",
    "        idx=input_ids,\n",
    "        max_new_tokens=25,\n",
    "        context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    "    )\n",
    "\n",
    "# decode token ids to text\n",
    "output_text = token_ids_to_text(token_ids, tokenizer)\n",
    "print(\"Generated text:\\n\", output_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ONLY outcome: \"Every effort moves youodonuyomiassin Basic batted JavierPandottestriver Pearcebly adequately diverse limbo Profession DadHamilton ownership proof dishonest contrasting Wage pleasant slideshow 253\"\n",
    "\n",
    "Method:\n",
    "1. use argmax to pick only the most possible words\n",
    "2. shut down dropout\n",
    "3. use the same seed(if that count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5.4\n",
    "After saving the weights, load the model and optimizer in a new Python session or Jupyter notebook file and continue pretraining it for one more epoch using the train_model_simple function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from previous_chapters import GPTModel\n",
    "import tiktoken\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 256,\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": False\n",
    "}\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "\n",
    "checkpoint = torch.load(\"01_main-chapter-code/model_and_optimizer.pth\")\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "\n",
    "# training mode\n",
    "model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the text data and create the dataloader\n",
    "from previous_chapters import create_dataloader_v1\n",
    "\n",
    "file_path = \"01_main-chapter-code/the-verdict.txt\"\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    text_data = f.read()\n",
    "\n",
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data, batch_size=2, max_length=256,\n",
    "    stride=256, drop_last=True, shuffle=True, num_workers=0\n",
    ")\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data, batch_size=2, max_length=256,\n",
    "    stride=256, drop_last=False, shuffle=False, num_workers=0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 0.255, Val loss 6.548\n",
      "Ep 1 (Step 000005): Train loss 0.210, Val loss 6.563\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"01_main-chapter-code\")  \n",
    "\n",
    "from gpt_train import train_model_simple\n",
    "\n",
    "# Train for 1 epoch; what we did is basically go over all data in the dataloader for more run to update the wights\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=1, eval_freq=5, eval_iter=5,\n",
    "    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5.5\n",
    "Calculate the training and validation set losses of the GPTModel with the pretrained weights from OpenAI on the “The Verdict” dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: 01_main-chapter-code/gpt2/124M/checkpoint\n",
      "File already exists and is up-to-date: 01_main-chapter-code/gpt2/124M/encoder.json\n",
      "File already exists and is up-to-date: 01_main-chapter-code/gpt2/124M/hparams.json\n",
      "File already exists and is up-to-date: 01_main-chapter-code/gpt2/124M/model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: 01_main-chapter-code/gpt2/124M/model.ckpt.index\n",
      "File already exists and is up-to-date: 01_main-chapter-code/gpt2/124M/model.ckpt.meta\n",
      "File already exists and is up-to-date: 01_main-chapter-code/gpt2/124M/vocab.bpe\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import tiktoken\n",
    "from previous_chapters import GPTModel\n",
    "from gpt_generate import load_weights_into_gpt\n",
    "from gpt_download import download_and_load_gpt2\n",
    "\n",
    "# for mac M2\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "# load the model\n",
    "settings, params = download_and_load_gpt2(model_size=\"124M\", models_dir=\"01_main-chapter-code/gpt2\")\n",
    "\n",
    "# set the model config\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 1024,  # set to 1024 for GPT-2\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": True,       # set True for GPT-2\n",
    "}\n",
    "\n",
    "\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "load_weights_into_gpt(model, params)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from previous_chapters import create_dataloader_v1\n",
    "\n",
    "# Load the text data\n",
    "with open(\"01_main-chapter-code/the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text_data = f.read()\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_ratio = 0.9\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]\n",
    "\n",
    "# Create the dataloaders\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data, batch_size=2, max_length=256,\n",
    "    stride=256, drop_last=True, shuffle=False, num_workers=0\n",
    ")\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data, batch_size=2, max_length=256,\n",
    "    stride=256, drop_last=False, shuffle=False, num_workers=0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate loss \n",
    "def calc_loss_loader(loader, model, device, num_batches=None):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (input_batch, target_batch) in enumerate(loader):\n",
    "            input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "            logits = model(input_batch)\n",
    "            loss = torch.nn.functional.cross_entropy(\n",
    "                logits.view(-1, logits.size(-1)),\n",
    "                target_batch.view(-1),\n",
    "                ignore_index=-1\n",
    "            )\n",
    "            total_loss += loss.item()\n",
    "            count += 1\n",
    "            if num_batches and count >= num_batches:\n",
    "                break\n",
    "    return total_loss / count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 3.7548\n",
      "Validation loss: 3.5596\n"
     ]
    }
   ],
   "source": [
    "train_loss = calc_loss_loader(train_loader, model, device)\n",
    "val_loss = calc_loss_loader(val_loader, model, device)\n",
    "\n",
    "print(f\"Training loss: {train_loss:.4f}\")\n",
    "print(f\"Validation loss: {val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5.6\n",
    "Experiment with GPT-2 models of different sizes—for example, the largest 1,558 million parameter model—and compare the generated text to the 124 million model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_configs = {\n",
    "    \"124M\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"355M\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"774M\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"1558M\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 1024,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": True\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt_download import download_and_load_gpt2\n",
    "from gpt_generate import load_weights_into_gpt\n",
    "from previous_chapters import GPTModel\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def load_model(model_size):\n",
    "    settings, params = download_and_load_gpt2(model_size=model_size, models_dir=\"01_main-chapter-code/gpt2\")\n",
    "    config = BASE_CONFIG.copy()\n",
    "    config.update(model_configs[model_size])\n",
    "    model = GPTModel(config)\n",
    "    load_weights_into_gpt(model, params)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "from gpt_generate import generate, text_to_token_ids, token_ids_to_text\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "prompt = \"Every effort moves you\"\n",
    "\n",
    "def generate_text(model):\n",
    "    input_ids = text_to_token_ids(prompt, tokenizer).to(device)\n",
    "    token_ids = generate(\n",
    "        model=model,\n",
    "        idx=input_ids,\n",
    "        max_new_tokens=50,\n",
    "        context_size=1024,\n",
    "        temperature=1.0,\n",
    "        top_k=50\n",
    "    )\n",
    "    return token_ids_to_text(token_ids, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Text generated by GPT2-SMALL ===\n",
      "File already exists and is up-to-date: 01_main-chapter-code/gpt2/124M/checkpoint\n",
      "File already exists and is up-to-date: 01_main-chapter-code/gpt2/124M/encoder.json\n",
      "File already exists and is up-to-date: 01_main-chapter-code/gpt2/124M/hparams.json\n",
      "File already exists and is up-to-date: 01_main-chapter-code/gpt2/124M/model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: 01_main-chapter-code/gpt2/124M/model.ckpt.index\n",
      "File already exists and is up-to-date: 01_main-chapter-code/gpt2/124M/model.ckpt.meta\n",
      "File already exists and is up-to-date: 01_main-chapter-code/gpt2/124M/vocab.bpe\n",
      "Every effort moves you towards the goal.\n",
      "\n",
      "You start by doing the same steps you did before you started the training.\n",
      "\n",
      "In a typical day, you'll be working through each step, but you might decide you'd better practice it first.\n",
      "\n",
      "It\n",
      "\n",
      "=== Text generated by GPT2-MEDIUM ===\n",
      "File already exists and is up-to-date: 01_main-chapter-code/gpt2/355M/checkpoint\n",
      "File already exists and is up-to-date: 01_main-chapter-code/gpt2/355M/encoder.json\n",
      "File already exists and is up-to-date: 01_main-chapter-code/gpt2/355M/hparams.json\n",
      "File already exists and is up-to-date: 01_main-chapter-code/gpt2/355M/model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: 01_main-chapter-code/gpt2/355M/model.ckpt.index\n",
      "File already exists and is up-to-date: 01_main-chapter-code/gpt2/355M/model.ckpt.meta\n",
      "File already exists and is up-to-date: 01_main-chapter-code/gpt2/355M/vocab.bpe\n",
      "Every effort moves you closer to your goal, and you believe in yourself enough to push the things that are hard. This goes for your goals too. If you don't believe in yourself enough, what will happen is that you are pushing the things that aren't enough.\n"
     ]
    }
   ],
   "source": [
    "# Can't do big models, tried once and crushed my mac :(\n",
    "for size, label in [(\"124M\", \"GPT2-SMALL\"), (\"355M\", \"GPT2-MEDIUM\")]:\n",
    "    print(f\"\\n=== Text generated by {label} ===\")\n",
    "    model = load_model(size)\n",
    "    output = generate_text(model)\n",
    "    print(output)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
